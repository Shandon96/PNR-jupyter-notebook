{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Max Runtime flow\n",
    "## Import relevant libraries and dataset\n",
    " - Dataset is created by PNRdatabase jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "data = pd.read_csv('PNRdatabase3.csv') # load dataset (csv file) with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features (X) and targets (y)\n",
    " - Define X as feature columns\n",
    " - Define y as target column RunTimeRoute\n",
    " - Define z as target column RunTimePlace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns which will not be used in machine learning model, leaving feature and target columns\n",
    "train = data.drop(['Type40LP','MetalStack','Date','RowDirection','DoubleBack','FlipFirstRow','StartfromFirstRow'],axis=1)\n",
    "\n",
    "# Drop target columns, leaving feature columns\n",
    "X = train.drop(['Violations','AntennaViolations','RunTimeRoute','RunTimePlace'],axis=1)\n",
    "\n",
    "# Define y and z to be target columns\n",
    "y = train['RunTimeRoute']\n",
    "z = train['RunTimePlace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from dataset (as known from PNRdatabase Machine Learning (Run Time) jupyter notebook)\n",
    "y_outliers = [83,67,87]\n",
    "z_outliers = [21,27,76,85,91]\n",
    "X_remove_outliers_Place = X.drop(z_outliers)\n",
    "X_remove_outliers_Route = X.drop(y_outliers)\n",
    "y_remove_outliers = y.drop(y_outliers)\n",
    "z_remove_outliers = z.drop(z_outliers)\n",
    "\n",
    "# Split dataset into train (80%) and test (20%) sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_Route, X_test_Route, y_train, y_test = train_test_split(X_remove_outliers_Route, y_remove_outliers, test_size=0.2, random_state=41)\n",
    "X_train_Place, X_test_Place, z_train, z_test = train_test_split(X_remove_outliers_Place, z_remove_outliers, test_size=0.2, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check each layer count for maximum run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[166, 171, 170, 173]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define an empty list to append values of maximum run time for each value of layer count\n",
    "listMaxRunTimeRoute = []\n",
    "\n",
    "# loop through each value of layer count from 6 to 9\n",
    "for i in range(6,10):\n",
    "    listRunTimeRoute = [] # define an empty list to store all values of run time for a specific layer count\n",
    "    layercount = np.where(X_train_Route['layercount'] == i)\n",
    "    for j in range(len(layercount[0])):\n",
    "        listRunTimeRoute.append(y_train.iloc[layercount[0][j]])\n",
    "    listMaxRunTimeRoute.append((max(listRunTimeRoute))) # append maximum value of run time from list of all values of run time\n",
    "listMaxRunTimeRoute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[195, 224, 217, 230]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define an empty list to append values of maximum run time for each value of layer count\n",
    "listMaxRunTimePlace = []\n",
    "\n",
    "# loop through each value of layer count from 6 to 9\n",
    "for i in range(6,10):\n",
    "    listRunTimePlace = [] # define an empty list to store all values of run time for a specific layer count\n",
    "    layercount = np.where(X_train_Place['layercount'] == i)\n",
    "    for j in range(len(layercount[0])):\n",
    "        listRunTimePlace.append(z_train.iloc[layercount[0][j]])\n",
    "    listMaxRunTimePlace.append((max(listRunTimePlace))) # append maximum value of run time from list of all values of run time\n",
    "listMaxRunTimePlace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check test set and mark failures\n",
    " - check actual run time values of test set and compare with benchmarked maximum values of run times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 91, 85]\n",
      "[99, 66, 155, 69, 68, 84]\n",
      "[76, 68, 92, 81, 60, 68, 59]\n",
      "[67, 160, 74, 60, 61]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a list to store all actual values of outcomes (failure (0) or success (1))\n",
    "listsuccessRoute = []\n",
    "\n",
    "k = 0\n",
    "for i in range(6,10):\n",
    "    listRunTimeRoute = []\n",
    "    layercount = np.where(X_test_Route['layercount'] == i)\n",
    "    for j in range(len(layercount[0])):\n",
    "        listRunTimeRoute.append(y_test.iloc[layercount[0][j]])\n",
    "        \n",
    "        # if run time is larger than maximum run time of same layer count, append 0 (failure) to list\n",
    "        if y_test.iloc[layercount[0][j]] > listMaxRunTimeRoute[k]:\n",
    "            listsuccessRoute.append(0)\n",
    "            \n",
    "        # if run time is smaller than maximum run time of same layer count, append 1 (success) to list\n",
    "        else:\n",
    "            listsuccessRoute.append(1)\n",
    "    print(listRunTimeRoute)\n",
    "    k = k+1\n",
    "listsuccessRoute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168, 198, 157, 181]\n",
      "[186, 170, 207, 156, 201]\n",
      "[213, 179, 167, 168, 205, 156]\n",
      "[198, 172, 209, 159, 174, 161]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a list to store all actual values of outcomes (failure (0) or success (1))\n",
    "listsuccessPlace = []\n",
    "k = 0\n",
    "for i in range(6,10):\n",
    "    listRunTimePlace = []\n",
    "    layercount = np.where(X_test_Place['layercount'] == i)\n",
    "    for j in range(len(layercount[0])):\n",
    "        listRunTimePlace.append(z_test.iloc[layercount[0][j]])\n",
    "        \n",
    "        # if run time is larger than maximum run time of same layer count, append 0 (failure) to list\n",
    "        if z_test.iloc[layercount[0][j]] > listMaxRunTimePlace[k]:\n",
    "            listsuccessPlace.append(0)\n",
    "            \n",
    "        # if run time is smaller than maximum run time of same layer count, append 1 (success) to list\n",
    "        else:\n",
    "            listsuccessPlace.append(1)\n",
    "    print(listRunTimePlace)\n",
    "    k = k+1\n",
    "listsuccessPlace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use best model with tuned hyperparameters to predict runtimes\n",
    " - As known from PNRdatabase Machine Learning (Run Time) jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 70.57045018  75.79877703 119.383977    78.69720103 100.82259223\n",
      "  84.37454368  75.13986682  82.75121557  75.519549    71.59728372\n",
      "  82.75121557 134.46021541  67.67263428  73.80487486  60.50840592\n",
      "  76.79970885  81.42853207  87.82309196  76.79970885  74.05359907\n",
      "  87.82309196]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "# fit best model with tuned hyperparameters using train set and test using test set\n",
    "XGB_test = GradientBoostingRegressor(max_features=1,max_depth=3,min_samples_leaf=7,random_state=7)\n",
    "XGB_test.fit(X_train_Route,y_train)\n",
    "y_pred = XGB_test.predict(X_test_Route)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181.16823032 181.10339426 180.8151207  181.11708823 180.77546289\n",
      " 180.74188969 180.75154309 181.09838766 181.02892396 181.11516596\n",
      " 181.10203349 180.77842215 180.78756218 180.79604259 180.77884552\n",
      " 180.72982106 180.77273866 181.08995836 180.80602596 180.74577163\n",
      " 181.10937313]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# fit best model with tuned hyperparameters using train set and test using test set\n",
    "svr_test = SVR(C=1,max_iter=10000)\n",
    "svr_test.fit(X_train_Place,z_train)\n",
    "z_pred = svr_test.predict(X_test_Place)\n",
    "print(z_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check predicted successes and failures\n",
    " - Note: this is different from above (Check test set and mark failures)\n",
    " - The marking of failure and success is dependent on predict values of run times of test set instead of actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78.69720103246648, 100.8225922340301, 71.59728372246197]\n",
      "[70.57045018476822, 75.79877702656844, 119.38397700098069, 84.37454367673024, 76.79970885207403, 76.79970885207403]\n",
      "[75.13986681932349, 82.75121556708143, 82.75121556708143, 67.6726342799172, 87.82309196185261, 74.05359907280248, 87.82309196185261]\n",
      "[75.5195489985331, 134.46021540633927, 73.80487485968364, 60.508405916577665, 81.42853207310013]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a list to store all PREDICTED values of outcomes (failure (0) or success (1))\n",
    "listPredictedSuccessRoute = []\n",
    "k = 0\n",
    "for i in range(6,10):\n",
    "    listRunTimeRoute = []\n",
    "    layercount = np.where(X_test_Route['layercount'] == i)\n",
    "    for j in range(len(layercount[0])):\n",
    "        listRunTimeRoute.append(y_pred[layercount[0][j]])\n",
    "        \n",
    "        # if run time is larger than maximum run time of same layer count, append 0 (failure) to list\n",
    "        if y_pred[layercount[0][j]] > listMaxRunTimeRoute[k]:\n",
    "            listPredictedSuccessRoute.append(0)\n",
    "            \n",
    "        # if run time is larger than maximum run time of same layer count, append 1 (success) to list\n",
    "        else:\n",
    "            listPredictedSuccessRoute.append(1)\n",
    "    print(listRunTimeRoute)\n",
    "    k = k+1\n",
    "listPredictedSuccessRoute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181.1170882252184, 180.77546289391577, 181.0289239632916, 180.77273866131355]\n",
      "[181.1682303246939, 180.81512070463347, 181.11516595773946, 180.72982105691963, 180.80602595761562]\n",
      "[181.10339425915402, 180.74188969302028, 181.10203349083616, 180.77884552494504, 181.089958360625, 180.7457716303632]\n",
      "[180.751543086358, 181.09838765978063, 180.77842215499797, 180.78756217507708, 180.7960425892777, 181.10937312750812]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a list to store all PREDICTED values of outcomes (failure (0) or success (1))\n",
    "listPredictedSuccessPlace = []\n",
    "k = 0\n",
    "for i in range(6,10):\n",
    "    listRunTimePlace = []\n",
    "    layercount = np.where(X_test_Place['layercount'] == i)\n",
    "    for j in range(len(layercount[0])):\n",
    "        listRunTimePlace.append(z_pred[layercount[0][j]])\n",
    "        \n",
    "        # if run time is larger than maximum run time of same layer count, append 0 (failure) to list\n",
    "        if y_pred[layercount[0][j]] > listMaxRunTimeRoute[k]:\n",
    "            listPredictedSuccessPlace.append(0)\n",
    "            \n",
    "        # if run time is smaller than maximum run time of same layer count, append 1 (success) to list\n",
    "        else:\n",
    "            listPredictedSuccessPlace.append(1)\n",
    "    print(listRunTimePlace)\n",
    "    k = k+1\n",
    "listPredictedSuccessPlace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy of predicted success/failure\n",
    " - Compare predicted list of success and failures with actual list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "accuracy_score_Place = accuracy_score(listsuccessPlace,listPredictedSuccessPlace)\n",
    "print(listsuccessPlace)\n",
    "print(listPredictedSuccessPlace)\n",
    "print(accuracy_score_Place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy_score_Route = accuracy_score(listsuccessRoute,listPredictedSuccessRoute)\n",
    "print(listsuccessRoute)\n",
    "print(listPredictedSuccessRoute)\n",
    "print(accuracy_score_Route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 0, 20]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix of RunTimePlace\n",
    "confusion_matrix(listsuccessPlace,listPredictedSuccessPlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix of RunTimeRoute\n",
    "confusion_matrix(listsuccessRoute,listPredictedSuccessRoute)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
