{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning flow (To predict RunTime)\n",
    "## Import relevant libraries and dataset\n",
    " - Dataset is created by PNRdatabase jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File PNRdatabase3.csv does not exist: 'PNRdatabase3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5c22def45aef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PNRdatabase3.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# load dataset (csv file) with pandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# ignore unimportant warnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File PNRdatabase3.csv does not exist: 'PNRdatabase3.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "data = pd.read_csv('PNRdatabase.csv') # load dataset (csv file) with pandas\n",
    "\n",
    "# ignore unimportant warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features (X) and targets (y/z)\n",
    " - Define X as feature columns\n",
    " - Define y as target column RunTimeRoute\n",
    " - Define z as target column RunTimePlace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns which will not be used in machine learning model, leaving feature and target columns\n",
    "train = data.drop(['Violations','AntennaViolations','Type40LP','MetalStack','Date','RowDirection','DoubleBack','FlipFirstRow','StartfromFirstRow'],axis=1)\n",
    "\n",
    "# Drop target columns, leaving feature columns\n",
    "X = train.drop(['RunTimeRoute','RunTimePlace'],axis=1)\n",
    "\n",
    "# Define y and z to be target columns\n",
    "y = train['RunTimeRoute']\n",
    "z = train['RunTimePlace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at distribution of targets\n",
    " - Check for presence of possible outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "fig,(ax1,ax2)=plt.subplots(1,2)\n",
    "ax1.hist(y, range=(0, 500), bins=20)\n",
    "ax2.hist(z, range=(100, 350), bins=20)\n",
    "ax1.set_title('Distribution of RunTimeRoute')\n",
    "ax2.set_title('Distribution of RunTimePlace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Outliers\n",
    " - seaborn boxplot function shows distribution of data\n",
    " - box shows quartiles of dataset while whiskers extend to the rest of the distribution, excluding points deemed as outliers by the function of the inter-quartile range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive inter-quartile ranges of both RunTimeRoute and RunTimePlace\n",
    "Q1_y = y.quantile(0.25)\n",
    "Q3_y = y.quantile(0.75)\n",
    "IQR_y = Q3_y - Q1_y\n",
    "Q1_z = z.quantile(0.25)\n",
    "Q3_z = z.quantile(0.75)\n",
    "IQR_z = Q3_z - Q1_z\n",
    "print(IQR_y,IQR_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all possible outliers of RunTimeRoute which are outside inter-quartile range\n",
    "y_outliers = []\n",
    "for i in range(len(y)):\n",
    "    if y[i] < (Q1_y - 1.5 * IQR_y) or y[i] > (Q3_y + 1.5 * IQR_y):\n",
    "        y_outliers.append(i)\n",
    "        print(i,y[i])\n",
    "print(len(y_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_outliers = [83,67,87] # outliers do not include small cluster of data points on the right of box (see box plot of RunTimeRoute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all possible outliers of RunTimePlace which are outside inter-quartile range\n",
    "z_outliers = []\n",
    "for i in range(len(z)):\n",
    "    if z[i] < (Q1_z - 1.5 * IQR_z) or z[i] > (Q3_z + 1.5 * IQR_z):\n",
    "        z_outliers.append(i)\n",
    "print(z_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers\n",
    " - Plot histogram of distribution excluding outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop outliers from all points\n",
    "X_remove_outliers_route = X.drop(y_outliers)\n",
    "X_remove_outliers_place = X.drop(z_outliers)\n",
    "y_remove_outliers = y.drop(y_outliers)\n",
    "z_remove_outliers = z.drop(z_outliers)\n",
    "\n",
    "# plot histogram of distribution excluding outliers\n",
    "sns.set_style('whitegrid')\n",
    "fig,(ax1,ax2)=plt.subplots(1,2)\n",
    "ax1.hist(y_remove_outliers, range=(0, 500), bins=20)\n",
    "ax2.hist(z_remove_outliers, range=(100, 350), bins=20)\n",
    "ax1.set_title('Distribution of RunTimeRoute')\n",
    "ax2.set_title('Distribution of RunTimePlace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data points to a normal distribution with mean 0 and variance 1\n",
    " - Necessary to ensure model assigns fair weight to features, without being affected by varying ranges and means of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas Dataframes to numpy arrays\n",
    "# necessary to input into sklearn's StandardScaler\n",
    "X_remove_outliers_route = X_remove_outliers_route.to_numpy()\n",
    "X_remove_outliers_place = X_remove_outliers_place.to_numpy()\n",
    "y_remove_outliers = y_remove_outliers.to_numpy()\n",
    "z_remove_outliers = z_remove_outliers.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X_place = StandardScaler()\n",
    "sc_X_route = StandardScaler()\n",
    "sc_Y = StandardScaler()\n",
    "sc_Z = StandardScaler()\n",
    "X_remove_outliers_place = sc_X_place.fit_transform(X_remove_outliers_place)\n",
    "X_remove_outliers_route = sc_X_route.fit_transform(X_remove_outliers_route)\n",
    "y_remove_outliers = sc_Y.fit_transform(y_remove_outliers.reshape(-1,1))\n",
    "z_remove_outliers = sc_Z.fit_transform(z_remove_outliers.reshape(-1,1))\n",
    "print(y_remove_outliers) # check that scaled RunTimeRoutes have a mean of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy arrays back to pandas Dataframes\n",
    "X_remove_outliers_route = pd.DataFrame(X_remove_outliers_route)\n",
    "X_remove_outliers_place = pd.DataFrame(X_remove_outliers_place)\n",
    "y_remove_outliers = pd.DataFrame(y_remove_outliers)\n",
    "z_remove_outliers = pd.DataFrame(z_remove_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach column names to pandas Dataframes (column names are lost when converted to numpy arrays)\n",
    "X_remove_outliers_route.columns = ['layercount', 'LayerM1', 'LayerM2', 'LayerM3', 'LayerM4', 'LayerM5',\n",
    "       'LayerM6', 'LayerM7', 'LayerL1', 'LayerL2', 'LayerL3', 'LayerL4',\n",
    "       'LayerBA', 'LayerBB', 'LayerBD', 'LayerBE', 'LayerBG', 'LayerFA',\n",
    "       'LayerFB', 'LayerU3T', 'LayerU3A', 'LayerLB', 'ViaCounts', 'NetCount',\n",
    "       'RowCount', 'CoreUtil', 'CoreWidth', 'CoreHeight', 'AspectRatio']\n",
    "X_remove_outliers_place.columns = ['layercount', 'LayerM1', 'LayerM2', 'LayerM3', 'LayerM4', 'LayerM5',\n",
    "       'LayerM6', 'LayerM7', 'LayerL1', 'LayerL2', 'LayerL3', 'LayerL4',\n",
    "       'LayerBA', 'LayerBB', 'LayerBD', 'LayerBE', 'LayerBG', 'LayerFA',\n",
    "       'LayerFB', 'LayerU3T', 'LayerU3A', 'LayerLB', 'ViaCounts', 'NetCount',\n",
    "       'RowCount', 'CoreUtil', 'CoreWidth', 'CoreHeight', 'AspectRatio']\n",
    "y_remove_outliers.columns = ['RunTimeRoute']\n",
    "z_remove_outliers.columns = ['RunTimePlace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split dataset into train set (80%) and test set (20%)\n",
    "X_train_route, X_test_route, y_train, y_test = train_test_split(X_remove_outliers_route, y_remove_outliers, test_size=0.2,random_state=41)\n",
    "X_train_place, X_test_place, z_train, z_test = train_test_split(X_remove_outliers_place, z_remove_outliers, test_size=0.2,random_state=41)\n",
    "\n",
    "# check shape of pandas Dataframes (that train set is ~80% and test set is 20%)\n",
    "print('X route shape:',X_remove_outliers_place.shape)\n",
    "print('X place shape:',X_remove_outliers_route.shape)\n",
    "print('y shape:',y_remove_outliers.shape)\n",
    "print('z shape:',z_remove_outliers.shape)\n",
    "print('X train route, X test route:',X_train_route.shape,X_test_route.shape)\n",
    "print('X train place, X test place:',X_train_place.shape,X_test_place.shape)\n",
    "print('y train, y test:',y_train.shape,y_test.shape)\n",
    "print('z train, z test:',z_train.shape,z_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the best features with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# function to return features with highest correlation to target\n",
    "def select_columns(feature_cols,target_col,no_features=5):\n",
    "    selector = SelectKBest(f_classif, k=no_features)\n",
    "    X_new = selector.fit_transform(feature_cols,target_col)\n",
    "    selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=train.index, \n",
    "                                 columns=X.columns)\n",
    "    selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "    return selected_columns\n",
    "\n",
    "selected_columns_route = select_columns(train[X.columns],train['RunTimeRoute'],7)\n",
    "selected_columns_place = select_columns(train[X.columns],train['RunTimePlace'],11)\n",
    "\n",
    "# print list of features with highest correlation to target\n",
    "print(selected_columns_place)\n",
    "print(selected_columns_route)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best regression model from scikit-learn using k-fold cross validation\n",
    "- Metrics used: root mean squared error\n",
    "- number of folds: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression    \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor   \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "k=7\n",
    "\n",
    "# list of regression models from sklearn\n",
    "Regressors = [\n",
    "    LinearRegression(),\n",
    "    Ridge(max_iter=10000,random_state=k),\n",
    "    Lasso(max_iter=10000,random_state=k),\n",
    "    ElasticNet(max_iter=10000,random_state=k),\n",
    "    KNeighborsRegressor(),\n",
    "    DecisionTreeRegressor(random_state=k),\n",
    "    SVR(max_iter=10000),\n",
    "    GradientBoostingRegressor(random_state=k),\n",
    "    RandomForestRegressor(random_state=k)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# initialize a high best score for RunTimeRoute and RunTimePlace\n",
    "best_score_route = 100\n",
    "best_score_place = 100\n",
    "\n",
    "# loop through regression model list to find the best scores\n",
    "for regressor in Regressors:\n",
    "    cv_results_route = -cross_val_score(regressor, X_train_route[selected_columns_route], y_train.values.ravel(), scoring='neg_root_mean_squared_error', cv=10 )\n",
    "    cv_results_place = -cross_val_score(regressor, X_train_place[selected_columns_place], z_train.values.ravel(), scoring='neg_root_mean_squared_error', cv=10 )\n",
    "    if best_score_route > cv_results_route.mean():\n",
    "        best_score_route = cv_results_route.mean()\n",
    "        best_regressor_route = regressor\n",
    "    if best_score_place > cv_results_place.mean():\n",
    "        best_score_place = cv_results_place.mean()\n",
    "        best_regressor_place = regressor\n",
    "\n",
    "# print best score and regression model\n",
    "print('best score (route) is: ',best_score_route)\n",
    "print('best regressor (route) is: ',best_regressor_route)\n",
    "        \n",
    "print('best score (place) is: ',best_score_place)\n",
    "print('best regressor (place) is: ',best_regressor_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model's hyperparameters with GridSearchCV/RandomizedSearchCV\n",
    " - After getting best values of hyperparameters, train and test best models with best values of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# define values of hyperparameters to be used in RandomizedSearchCv\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(0, 6),\n",
    "              \"min_samples_leaf\": randint(1, 9)}\n",
    "\n",
    "# fit model with train set and carry out RandomizedSearchCV for RunTimeRoute\n",
    "XGB = GradientBoostingRegressor(random_state=11)\n",
    "XGB_cv = RandomizedSearchCV(XGB, param_dist,scoring='neg_root_mean_squared_error', cv=10)\n",
    "XGB_cv.fit(X_train_route[selected_columns_route],y_train.values.ravel())\n",
    "print(XGB_cv.best_estimator_)\n",
    "print(-XGB_cv.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Use tuned (best values) hyperparameters to train model with train set \n",
    "XGB_test = GradientBoostingRegressor(max_features=1,max_depth=3,min_samples_leaf=7,random_state=7)\n",
    "XGB_test.fit(X_train_route,y_train.values.ravel())\n",
    "y_pred = XGB_test.predict(X_test_route) # test model with test set\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph of predited and actual values of RunTimeRoute\n",
    "testlist = list(range(1,22))\n",
    "plt.plot(testlist, y_pred)\n",
    "plt.plot(testlist, y_test)\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('y_pred/ y_test')\n",
    "plt.legend(['y_pred','y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define values of hyperparameters to be used in GridSearchCV\n",
    "C_range = [1,10,100,1000,10000]\n",
    "param_grid = dict(C= C_range)\n",
    "svr = SVR(max_iter=10000)\n",
    "\n",
    "# fit model with train set and carry out RandomizedSearchCV for RunTimePlace\n",
    "search = GridSearchCV(estimator = svr, param_grid = param_grid, scoring='neg_root_mean_squared_error', cv=10)\n",
    "search.fit(X_train_place[selected_columns_route],z_train.values.ravel())\n",
    "print(search.best_estimator_)\n",
    "print(-search.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tuned (best values) hyperparameters to train model with train set \n",
    "svr_test = SVR(C=1,max_iter=10000)\n",
    "svr_test.fit(X_train_place,z_train.values.ravel())\n",
    "z_pred = svr_test.predict(X_test_place)\n",
    "print(z_pred)\n",
    "print(z_test)\n",
    "print(np.sqrt(metrics.mean_squared_error(z_test, z_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph of predited and actual values of RunTimePlace\n",
    "testlist = list(range(1,22))\n",
    "plt.plot(testlist, z_pred)\n",
    "plt.plot(testlist, z_test)\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('z_pred/z_test')\n",
    "plt.legend(['z_pred','z_test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
